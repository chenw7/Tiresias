{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.stats import zscore, median_abs_deviation\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "\n",
    "# 1. File Paths\n",
    "MODEL_PATH = 'V:/SimbaChen/xgb_raw_noimpute_removal.joblib'\n",
    "PIPELINE_PATH = 'V:/SimbaChen/pipeline_noimpute_removal.joblib'\n",
    "\n",
    "# 2. Key Column Names\n",
    "GROUPBY_COLS = ['Lot', 'Wafer Number']\n",
    "SITE_COL = 'site'\n",
    "\n",
    "# 3. Preprocessing Parameters\n",
    "Z_SCORE_THRESHOLD = 5.0 # Threshold for replacing outliers with NaN in preprocess_data\n",
    "\n",
    "# 4. Anomaly Detection Parameters\n",
    "ANOMALY_SCORE_THRESHOLD = -0.65 # Threshold for Isolation Forest in detect_anomalies\n",
    "\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by dropping irrelevant columns and handling outliers.\n",
    "\n",
    "    Args:\n",
    "        df: The raw input DataFrame for a single Lot/Wafer group.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The preprocessed DataFrame.\n",
    "        - A DataFrame of median values per test, with sites as columns.\n",
    "        - A DataFrame of standard deviation values per test, with sites as columns.\n",
    "    \"\"\"\n",
    "    # Define columns and patterns to drop\n",
    "    prefixes_to_drop = [\n",
    "        'ECID', 'failtest_name', 'failtest_number', 'hardbin_name', 'hardbin_number',\n",
    "        'lastfailtest_name', 'lastfailtest_number', 'Part_Id', 'softbin_name', 'softbin_number',\n",
    "        'Lot', 'DieID', 'CodedDieID', 'OCR', 'Source Lot', 'rework_flag', 'Program', 'Wafer Number',\n",
    "        'start_time', 'temperature', 'subid', 'Wafer','die_x', 'die_y', 'device_nr', 'rom_code',\n",
    "        'hardbin', 'lib_info', 'BinName', 'BinState', 'cnt_alarmed_tests', 'cnt_failed_tests',\n",
    "        'cnt_passed_tests', 'cnt_recorded_tests', 'Device_Test_Time', 'lot_key', 'wf_key', 'Fab',\n",
    "        'Probecard', 'Testcenter', 'Tester', 'program revision', 'Stage',\n",
    "    ]\n",
    "    suffixes_to_drop = [',0,0', ',0,-']\n",
    "\n",
    "    # Build the list of columns to drop\n",
    "    cols_to_drop = [\n",
    "        col for col in df.columns\n",
    "        if any(col.startswith(p) for p in prefixes_to_drop) or any(col.endswith(s) for s in suffixes_to_drop)\n",
    "    ]\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # Identify numeric columns, excluding 'site'. This part is already efficient.\n",
    "    numeric_cols = df.columns.drop(SITE_COL)\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # --- Vectorized Outlier Removal ---\n",
    "    # 1. Calculate group-wise mean and std for all columns at once.\n",
    "    #    .transform() broadcasts the result back to the original DataFrame's shape.\n",
    "    means = df.groupby(SITE_COL)[numeric_cols].transform('mean')\n",
    "    stds = df.groupby(SITE_COL)[numeric_cols].transform('std')\n",
    "\n",
    "    # 2. Calculate Z-scores for all data points in a single, vectorized operation.\n",
    "    #    We'll fill stds of 0 with NaN to avoid division-by-zero errors, then fill\n",
    "    #    resulting Z-score NaNs with 0, as a point with no deviation is 0 Z-score.\n",
    "    z_scores = ((df[numeric_cols] - means) / stds.replace(0, np.nan)).fillna(0)\n",
    "    \n",
    "    # 3. Create a boolean mask to identify outliers.\n",
    "    #    This mask is True for any value where the absolute Z-score > 5.\n",
    "    mask = z_scores.abs() > 5\n",
    "\n",
    "    # 4. Use the mask to replace outliers with NaN.\n",
    "    #    df.where() keeps original values where the mask is False.\n",
    "    df[numeric_cols] = df[numeric_cols].where(~mask, np.nan)\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Calculate final median and standard deviation after handling outliers.\n",
    "    # These are already efficient groupby aggregations.\n",
    "    med = df.groupby(SITE_COL).median().T\n",
    "    std = df.groupby(SITE_COL).std().T\n",
    "    \n",
    "    return df, med, std\n",
    "\n",
    "\n",
    "def standardize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies row-wise Z-score standardization, with maximum safety checks for\n",
    "    data types and data variation to ensure universal compatibility.\n",
    "    \"\"\"\n",
    "    def safe_zscore(row):\n",
    "        \"\"\"\n",
    "        Calculates zscore by first sanitizing the row's data type, then checking\n",
    "        for variation, making it robust against all known edge cases.\n",
    "        \"\"\"\n",
    "        # ** THE FINAL, BULLETPROOF FIX **\n",
    "        # 1. Sanitize the input: Immediately convert the row to a numeric type.\n",
    "        #    This is the most critical step to prevent the TypeError.\n",
    "        numeric_row = pd.to_numeric(row, errors='coerce')\n",
    "\n",
    "        # 2. Work only with valid data from the sanitized row.\n",
    "        valid_data = numeric_row.dropna()\n",
    "        \n",
    "        # 3. Check for variation.\n",
    "        if valid_data.nunique() <= 1:\n",
    "            return pd.Series(0.0, index=numeric_row.index).where(numeric_row.notna())\n",
    "        \n",
    "        # 4. If all checks pass, perform the calculation on the sanitized data.\n",
    "        return zscore(numeric_row, nan_policy='omit')\n",
    "\n",
    "    # Apply the fully robust safe_zscore function to each row.\n",
    "    return df.apply(safe_zscore, axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "def generate_features(med: pd.DataFrame, std: pd.DataFrame, z_med: pd.DataFrame, z_std: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates statistical features for the machine learning model.\n",
    "\n",
    "    Args:\n",
    "        med: DataFrame of median values (tests x sites).\n",
    "        std: DataFrame of standard deviation values (tests x sites).\n",
    "        z_med: Standardized median values.\n",
    "        z_std: Standardized standard deviation values.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame of generated features, with one row per test parameter.\n",
    "    \"\"\"\n",
    "    features_dict = {\n",
    "        \"z_med_range\": z_med.max(axis=1) - z_med.min(axis=1),\n",
    "        \"z_std_range\": z_std.max(axis=1) - z_std.min(axis=1),\n",
    "        \"z_med_iqr\": z_med.quantile(0.75, axis=1) - z_med.quantile(0.25, axis=1),\n",
    "        \"z_std_iqr\": z_std.quantile(0.75, axis=1) - z_std.quantile(0.25, axis=1),\n",
    "        \"z_med_mad\": z_med.apply(median_abs_deviation, axis=1),\n",
    "        \"z_std_mad\": z_std.apply(median_abs_deviation, axis=1),\n",
    "        \"med_skewness\": med.skew(axis=1),\n",
    "        \"std_skewness\": std.skew(axis=1),\n",
    "        \"med_kurtosis\": med.kurt(axis=1),\n",
    "        \"std_kurtosis\": std.kurt(axis=1),\n",
    "    }\n",
    "    return pd.DataFrame(features_dict)\n",
    "\n",
    "\n",
    "def predict_anomalies(features: pd.DataFrame, model_path: str, pipeline_path: str) -> np.ndarray:\n",
    "    \"\"\"Loads a model and pipeline to predict anomalies from features.\"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    x_transformed = pipeline.transform(features)\n",
    "    return model.predict(x_transformed)\n",
    "    \n",
    "\n",
    "def detect_anomalies(z_med: pd.DataFrame, z_std: pd.DataFrame, predictions: np.ndarray, features_index: pd.Index) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters out non-anomalous tests and performs a second-level check for outlier sites using Isolation Forest scores.\n",
    "\n",
    "    Args:\n",
    "        z_med: Standardized median values (tests x sites).\n",
    "        z_std: Standardized standard deviation values (tests x sites).\n",
    "        predictions: Anomaly predictions from the XGBoost model.\n",
    "        features_index: The index of the features DataFrame (test names).\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame listing each anomalous site and its corresponding test parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_df = pd.DataFrame({'XGB_is_anomaly': predictions}, index=features_index)\n",
    "    bad_params_df = pred_df[pred_df['XGB_is_anomaly'] == 0]\n",
    "\n",
    "    z_med_filtered = z_med.loc[bad_params_df.index]\n",
    "    z_std_filtered = z_std.loc[bad_params_df.index]\n",
    "\n",
    "    anomalies_records = []\n",
    "\n",
    "    for test_name in z_med_filtered.index:\n",
    "        data = pd.DataFrame({\n",
    "            'med': z_med_filtered.loc[test_name],\n",
    "            'std': z_std_filtered.loc[test_name]\n",
    "        }).dropna()\n",
    "\n",
    "        if data.empty:\n",
    "            continue\n",
    "\n",
    "        iso_forest = IsolationForest(contamination='auto', random_state=42)\n",
    "        iso_forest.fit(data[['med', 'std']])\n",
    "        data['anomaly_score'] = iso_forest.score_samples(data[['med', 'std']])\n",
    "\n",
    "        # Flag sites with scores below the fixed threshold\n",
    "        anomalous_sites = data[data['anomaly_score'] < ANOMALY_SCORE_THRESHOLD].index.tolist()\n",
    "\n",
    "        for site in anomalous_sites:\n",
    "            anomalies_records.append({SITE_COL: site, 'test_param': test_name})\n",
    "\n",
    "    if not anomalies_records:\n",
    "        return pd.DataFrame(columns=[SITE_COL, 'test_param'])\n",
    "\n",
    "    return pd.DataFrame(anomalies_records).sort_values(by=SITE_COL, ascending=True)\n",
    "\n",
    "\n",
    "def main(df_in: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Main function to run the complete anomaly detection pipeline.\n",
    "\n",
    "    Args:\n",
    "        df_in: The raw input DataFrame containing data for multiple lots and wafers.\n",
    "    \"\"\"\n",
    "    all_anomalies = []\n",
    "\n",
    "    # Process each Lot/Wafer group individually\n",
    "    for (lot, wafer), group_df in df_in.groupby(GROUPBY_COLS):\n",
    "        print(f\"Processing Lot: {lot}, Wafer: {wafer}...\")\n",
    "        \n",
    "        # 1. Preprocess data and calculate stats\n",
    "        df, med, std = preprocess_data(group_df)\n",
    "        if med.empty or std.empty:\n",
    "            print(f\"  -> Skipping group due to empty data after preprocessing.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Standardize stats and generate features for the model\n",
    "        z_med = standardize(med)\n",
    "        z_std = standardize(std)\n",
    "        features = generate_features(med, std, z_med, z_std)\n",
    "\n",
    "        # 3. Predict which TEST PARAMETERS are anomalous\n",
    "        predictions = predict_anomalies(features, MODEL_PATH, PIPELINE_PATH)\n",
    "\n",
    "        # 4. For the non-anomalous tests, find which specific SITES are outliers\n",
    "        anomalies_df = detect_anomalies(z_med, z_std, predictions, features.index)\n",
    "\n",
    "        # If anomalies were found, add identifiers and store the result\n",
    "        if not anomalies_df.empty:\n",
    "            anomalies_df['Lot'] = lot\n",
    "            anomalies_df['Wafer Number'] = wafer\n",
    "            all_anomalies.append(anomalies_df)\n",
    "\n",
    "    # Combine results from all groups into a final DataFrame\n",
    "    if all_anomalies:\n",
    "        final_anomalies_df = pd.concat(all_anomalies, ignore_index=True)\n",
    "        # Reorder columns for the final output\n",
    "        final_anomalies_df = final_anomalies_df[['Lot', 'Wafer Number', SITE_COL, 'test_param']]\n",
    "        print(\"\\n--- Anomaly Detection Complete ---\")\n",
    "        print(final_anomalies_df)\n",
    "        return final_anomalies_df\n",
    "    else:\n",
    "        print(\"\\n--- Anomaly Detection Complete: No anomalies found. ---\")\n",
    "        columns=['Lot', 'Wafer Number', SITE_COL, 'test_param']\n",
    "        nan_df = pd.DataFrame([[np.nan] * len(columns)], columns=columns)\n",
    "        return nan_df\n",
    "\n",
    "df_in = pd.DataFrame(data=input)\n",
    "\n",
    "# Run the main pipeline\n",
    "anomalies_df = main(df_in)\n",
    "\n",
    "# Generating count per test parameter table\n",
    "test_param_counts = anomalies_df['test_param'].value_counts().reset_index()\n",
    "test_param_counts.columns = ['test_param', 'count']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
